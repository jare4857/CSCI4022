{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI4022 Homework 2; Review\n",
    "\n",
    "## Due Monday, February 8 at 11:59 pm to Canvas\n",
    "\n",
    "#### Submit this file as a .ipynb with *all cells compiled and run* to the associated dropbox.\n",
    "\n",
    "***\n",
    "\n",
    "Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Here is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.  I also recommend the [wikibook](https://en.wikibooks.org/wiki/LaTeX) for LaTex.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do **Kernel $\\rightarrow$ Restart & Run All** as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 45 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "- There is *not a prescribed API* for these problems.  You may answer coding questions with whatever syntax or object typing you deem fit.  Your evaluation will primarily live in the clarity of how well you present your final results, so don't skip over any interpretations!  Your code should still be commented and readable to ensure you followed the given course algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "# from cleantext import clean\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p1'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 1 (Theory: minhashing; 10 pts)\n",
    "\n",
    "Consider minhash values for a single column vector that contains 10 components/rows. Seven of rows hold 0 and three hold 1. Consider taking all 10! = 3,628,800 possible distinct permutations of ten rows. When we choose a permutation of the rows and produce a minhash value for the column, we will use the number of the row, in the permuted order, that is the first with a 1.  Use Markdown cells to demonstrate answers to the following.\n",
    "\n",
    "#### a) For exactly how many of the 3,628,800 permutations is the minhash value for the column a 9?  What proportion is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be $0$ permutations where the minhash value for the column is $9$. The proportion is also $0$. This is because there is no possible way for the 9th component to be the first $1$ to appear when there are $10$ components total with $3$ ones and $6$ zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) For exactly how many of the 3,628,800 permutations is the minhash value for the column a 8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be $1$ permutaion where the minhash value for the column is $8$. The proportion is $\\frac{1}{3,628,800}$. This is becauser there is only one possible way for the 8th component to be the first $1$ to appear when there are $10$ components total with $3$ ones and $6$ zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) For exactly how many of the 3,628,800 permutations is the minhash value for the column a 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be $7!$ or $5040$ permutations where the minhash value for the column is $3$. The proporiton is $\\frac{1}{720}$. This is because the first three components must be $001$ which leaves $7!$ possible permuations with this as the beggining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p3'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 2 (Applied Minhashing; 35 pts)\n",
    "\n",
    "In this problem we compare similarities of 5 documents available on http://www.gutenberg.org\n",
    "\n",
    " 1) The first approximately 10000 characters of Miguel de Unamuno's *Niebla*, written in Spanish, in the file `niebla.txt`\n",
    " \n",
    " 2) The first approximately 10000 characters of Miguel de Cervantes *The Ingenious Gentleman Don Quixote of La Mancha*, written in Spanish, in the file `DQ.txt`\n",
    " \n",
    " 3) The first approximately 10000 characters of Homer's *The Odyssey*, translated into English by Samuel Butler, in the file `odyssey.txt`\n",
    " \n",
    " 4) The first approximately 10000 characters of Kate Chopin's *The Awakening* in the file `awaken.txt`\n",
    " \n",
    " 5) The entirety of around 12000 characters of Kate Chopin's *Beyond the Bayou* in the file `BB.txt`\n",
    " \n",
    "### a) Clean the 4 documents, scrubbing all punctuation, changes cases to lower case, and removing accent marks as appropriate.  \n",
    "\n",
    "You should have only 27 unique characters in each book/section after cleaning, corresponding to white spaces and the 26 letters.  \n",
    "\n",
    "\n",
    "**For this problem, you may import any text-based packages you desire to help wrangle the data.**  I recommend looking at some functions within `string` or the RegEx `re` packages.\n",
    "\n",
    "You can and probably should use functions in the string package such as `string.lower`, `string.replace`, etc.\n",
    "\n",
    "All 5 documents have been saved in UTF-8 encoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1 = \"data/awaken.txt\"\n",
    "filename_2 = \"data/BB.txt\"\n",
    "filename_3 = \"data/DQ.txt\"\n",
    "filename_4 = \"data/niebla.txt\"\n",
    "filename_5 = \"data/odyssey.txt\"\n",
    "\n",
    "def strip_accents(s): #from this stack overflow post: https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "#for awaken.txt\n",
    "file = open(filename_1, 'rt', encoding=\"utf8\")\n",
    "text_1 = file.read()\n",
    "file.close\n",
    "\n",
    "text_1 = text_1.lower()\n",
    "text_1 = re.sub(r'[^\\w\\s]', '', text_1) #as seen here: https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "text_1 = text_1.translate({ord(k): None for k in string.digits})\n",
    "text_1 = re.sub(r'\\n', '', text_1)\n",
    "\n",
    "\n",
    "#for BB.txt\n",
    "file = open(filename_2, 'rt', encoding=\"utf8\")\n",
    "text_2 = file.read()\n",
    "file.close\n",
    "\n",
    "text_2 = text_2.lower()\n",
    "text_2 = re.sub(r'[^\\w\\s]', '', text_2) #as seen here: https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "text_2 = text_2.translate({ord(k): None for k in string.digits})\n",
    "text_2 = re.sub(r'\\n', '', text_2)\n",
    "\n",
    "\n",
    "#for DQ.txt\n",
    "file = open(filename_3, 'rt', encoding=\"utf8\")\n",
    "text_3 = file.read()\n",
    "file.close\n",
    "\n",
    "text_3 = text_3.lower()\n",
    "text_3 = re.sub(r'[^\\w\\s]', '', text_3) #as seen here: https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "text_3 = text_3.translate({ord(k): None for k in string.digits})\n",
    "text_3 = strip_accents(text_3)\n",
    "text_3 = re.sub(r'\\n', '', text_3)\n",
    "\n",
    "\n",
    "#for niebla.txt\n",
    "file = open(filename_4, 'rt', encoding=\"utf8\")\n",
    "text_4 = file.read()\n",
    "file.close\n",
    "\n",
    "text_4 = text_4.lower()\n",
    "text_4 = re.sub(r'[^\\w\\s]', '', text_4) #as seen here: https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "text_4 = re.sub(r'[_]', '', text_4)\n",
    "text_4 = text_4.translate({ord(k): None for k in string.digits})\n",
    "text_4 = strip_accents(text_4)\n",
    "text_4 = re.sub(r'\\n', '', text_4)\n",
    "\n",
    "\n",
    "#for odyssey.txt\n",
    "file = open(filename_5, 'rt', encoding=\"utf8\")\n",
    "text_5 = file.read()\n",
    "file.close\n",
    "\n",
    "text_5 = text_5.lower()\n",
    "text_5 = re.sub(r'[^\\w\\s]', '', text_5) #as seen here: https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "text_5 = text_5.translate({ord(k): None for k in string.digits})\n",
    "text_5 = re.sub(r'\\n', '', text_5)\n",
    "\n",
    "#print(text_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b) Compute exact similarity scores between the documents.  Are these the expected results?\n",
    "\n",
    "Notes:\n",
    "- You may choose or explore different values of $k$ for your shingles.\n",
    "- You may choose to shingle on words and create an n-gram model, but it is recommended you shingle on letters as described in class\n",
    "- You may construct your characteristic matrix or characteristic sets with or without hash functions (e.g. by using `set()`).  Note that choice of hash function should change heavily with $k$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "[[1.         1.         0.92592593 0.96296296 1.        ]\n",
      " [1.         1.         0.92592593 0.96296296 1.        ]\n",
      " [0.92592593 0.92592593 1.         0.96153846 0.92592593]\n",
      " [0.96296296 0.96296296 0.96153846 1.         0.96296296]\n",
      " [1.         1.         0.92592593 0.96296296 1.        ]]\n",
      "k = 2\n",
      "[[1.         0.8        0.56643357 0.61904762 0.75565611]\n",
      " [0.8        1.         0.55990783 0.63781321 0.7912844 ]\n",
      " [0.56643357 0.55990783 1.         0.78247734 0.54137116]\n",
      " [0.61904762 0.63781321 0.78247734 1.         0.5954023 ]\n",
      " [0.75565611 0.7912844  0.54137116 0.5954023  1.        ]]\n",
      "k = 3\n",
      "[[1.         0.50554734 0.28365759 0.31317886 0.47142309]\n",
      " [0.50554734 1.         0.27883539 0.3139657  0.48315868]\n",
      " [0.28365759 0.27883539 1.         0.47916667 0.26878728]\n",
      " [0.31317886 0.3139657  0.47916667 1.         0.30044346]\n",
      " [0.47142309 0.48315868 0.26878728 0.30044346 1.        ]]\n",
      "k = 4\n",
      "[[1.         0.28104002 0.07728803 0.09512027 0.2583423 ]\n",
      " [0.28104002 1.         0.07927179 0.09731413 0.27562877]\n",
      " [0.07728803 0.07927179 1.         0.25884335 0.07644596]\n",
      " [0.09512027 0.09731413 0.25884335 1.         0.09229702]\n",
      " [0.2583423  0.27562877 0.07644596 0.09229702 1.        ]]\n",
      "k = 5\n",
      "[[1.         0.16161988 0.01395582 0.01989008 0.14578698]\n",
      " [0.16161988 1.         0.01583502 0.01880094 0.15271514]\n",
      " [0.01395582 0.01583502 1.         0.13796599 0.01203749]\n",
      " [0.01989008 0.01880094 0.13796599 1.         0.01690697]\n",
      " [0.14578698 0.15271514 0.01203749 0.01690697 1.        ]]\n",
      "k = 6\n",
      "[[1.         0.1016767  0.00193892 0.00474198 0.08863501]\n",
      " [0.1016767  1.         0.00243291 0.00327932 0.08913899]\n",
      " [0.00193892 0.00243291 1.         0.07727738 0.00148647]\n",
      " [0.00474198 0.00327932 0.07727738 1.         0.00277665]\n",
      " [0.08863501 0.08913899 0.00148647 0.00277665 1.        ]]\n",
      "k = 10\n",
      "[[1.         0.01223776 0.         0.         0.00762341]\n",
      " [0.01223776 1.         0.         0.         0.00706611]\n",
      " [0.         0.         1.         0.00756657 0.        ]\n",
      " [0.         0.         0.00756657 1.         0.        ]\n",
      " [0.00762341 0.00706611 0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# k = 1\n",
    "awaken_set_1 = set(text_1)\n",
    "bb_set_1 = set(text_2)\n",
    "dq_set_1 = set(text_3)\n",
    "niebla_set_1 = set(text_4)\n",
    "odyssey_set_1 = set(text_5)\n",
    "\n",
    "# print(len(awaken_set))\n",
    "# print(len(bb_set))\n",
    "# print(len(dq_set)) #missing w and k\n",
    "# print(len(niebla_set)) #missing w\n",
    "#print(odyssey_set_1)\n",
    "\n",
    "\n",
    "\n",
    "# d = {'awaken': awaken_set, 'bb': bb_set, 'dq': dq_set, 'niebla': niebla_set, 'odyssey': odyssey_set}\n",
    "# df = pd.DataFrame(data = d)\n",
    "\n",
    "text_sets_1 = [awaken_set_1, bb_set_1, dq_set_1, niebla_set_1, odyssey_set_1]\n",
    "# awaken_sim_1 = []\n",
    "\n",
    "simularities_1 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_1)):\n",
    "    for j in range(len(text_sets_1)):\n",
    "        numer = len(text_sets_1[i].intersection(text_sets_1[j]))\n",
    "        denom = len(text_sets_1[i].union(text_sets_1[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_1[i][j] = sim\n",
    "\n",
    "print(\"k = 1\")\n",
    "print(simularities_1)\n",
    "\n",
    "\n",
    "#k = 2\n",
    "k = 2\n",
    "awaken_set_2 = set([text_1[i:i + k] for i in range(len(text_1) - k + 1)])\n",
    "bb_set_2 = set([text_2[i:i + k] for i in range(len(text_2) - k + 1)])\n",
    "dq_set_2 = set([text_3[i:i + k] for i in range(len(text_3) - k + 1)])\n",
    "niebla_set_2 = set([text_4[i:i + k] for i in range(len(text_4) - k + 1)])\n",
    "odyssey_set_2 = set([text_5[i:i + k] for i in range(len(text_5) - k + 1)])\n",
    "\n",
    "text_sets_2 = [awaken_set_2, bb_set_2, dq_set_2, niebla_set_2, odyssey_set_2]\n",
    "\n",
    "simularities_2 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_2)):\n",
    "    for j in range(len(text_sets_2)):\n",
    "        numer = len(text_sets_2[i].intersection(text_sets_2[j]))\n",
    "        denom = len(text_sets_2[i].union(text_sets_2[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_2[i][j] = sim\n",
    "\n",
    "print(\"k = 2\")\n",
    "print(simularities_2)        \n",
    "\n",
    "\n",
    "#k = 3\n",
    "k = 3\n",
    "awaken_set_3 = set([text_1[i:i + k] for i in range(len(text_1) - k + 1)])\n",
    "bb_set_3 = set([text_2[i:i + k] for i in range(len(text_2) - k + 1)])\n",
    "dq_set_3 = set([text_3[i:i + k] for i in range(len(text_3) - k + 1)])\n",
    "niebla_set_3 = set([text_4[i:i + k] for i in range(len(text_4) - k + 1)])\n",
    "odyssey_set_3 = set([text_5[i:i + k] for i in range(len(text_5) - k + 1)])\n",
    "\n",
    "text_sets_3 = [awaken_set_3, bb_set_3, dq_set_3, niebla_set_3, odyssey_set_3]\n",
    "\n",
    "simularities_3 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_3)):\n",
    "    for j in range(len(text_sets_3)):\n",
    "        numer = len(text_sets_3[i].intersection(text_sets_3[j]))\n",
    "        denom = len(text_sets_3[i].union(text_sets_3[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_3[i][j] = sim\n",
    "\n",
    "print(\"k = 3\")\n",
    "print(simularities_3)\n",
    "\n",
    "\n",
    "#k = 4\n",
    "k = 4\n",
    "awaken_set_4 = set([text_1[i:i + k] for i in range(len(text_1) - k + 1)])\n",
    "bb_set_4 = set([text_2[i:i + k] for i in range(len(text_2) - k + 1)])\n",
    "dq_set_4 = set([text_3[i:i + k] for i in range(len(text_3) - k + 1)])\n",
    "niebla_set_4 = set([text_4[i:i + k] for i in range(len(text_4) - k + 1)])\n",
    "odyssey_set_4 = set([text_5[i:i + k] for i in range(len(text_5) - k + 1)])\n",
    "\n",
    "text_sets_4 = [awaken_set_4, bb_set_4, dq_set_4, niebla_set_4, odyssey_set_4]\n",
    "\n",
    "simularities_4 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_4)):\n",
    "    for j in range(len(text_sets_4)):\n",
    "        numer = len(text_sets_4[i].intersection(text_sets_4[j]))\n",
    "        denom = len(text_sets_4[i].union(text_sets_4[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_4[i][j] = sim\n",
    "\n",
    "print(\"k = 4\")\n",
    "print(simularities_4)\n",
    "\n",
    "\n",
    "#k = 5\n",
    "k = 5\n",
    "awaken_set_5 = set([text_1[i:i + k] for i in range(len(text_1) - k + 1)])\n",
    "bb_set_5 = set([text_2[i:i + k] for i in range(len(text_2) - k + 1)])\n",
    "dq_set_5 = set([text_3[i:i + k] for i in range(len(text_3) - k + 1)])\n",
    "niebla_set_5 = set([text_4[i:i + k] for i in range(len(text_4) - k + 1)])\n",
    "odyssey_set_5 = set([text_5[i:i + k] for i in range(len(text_5) - k + 1)])\n",
    "\n",
    "text_sets_5 = [awaken_set_5, bb_set_5, dq_set_5, niebla_set_5, odyssey_set_5]\n",
    "\n",
    "simularities_5 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_5)):\n",
    "    for j in range(len(text_sets_5)):\n",
    "        numer = len(text_sets_5[i].intersection(text_sets_5[j]))\n",
    "        denom = len(text_sets_5[i].union(text_sets_5[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_5[i][j] = sim\n",
    "\n",
    "print(\"k = 5\")\n",
    "print(simularities_5)  \n",
    "\n",
    "\n",
    "#k = 6\n",
    "k = 6\n",
    "awaken_set_6 = set([text_1[i:i + k] for i in range(len(text_1) - k + 1)])\n",
    "bb_set_6 = set([text_2[i:i + k] for i in range(len(text_2) - k + 1)])\n",
    "dq_set_6 = set([text_3[i:i + k] for i in range(len(text_3) - k + 1)])\n",
    "niebla_set_6 = set([text_4[i:i + k] for i in range(len(text_4) - k + 1)])\n",
    "odyssey_set_6 = set([text_5[i:i + k] for i in range(len(text_5) - k + 1)])\n",
    "\n",
    "text_sets_6 = [awaken_set_6, bb_set_6, dq_set_6, niebla_set_6, odyssey_set_6]\n",
    "\n",
    "simularities_6 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_5)):\n",
    "    for j in range(len(text_sets_5)):\n",
    "        numer = len(text_sets_6[i].intersection(text_sets_6[j]))\n",
    "        denom = len(text_sets_6[i].union(text_sets_6[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_6[i][j] = sim\n",
    "\n",
    "print(\"k = 6\")\n",
    "print(simularities_6)  \n",
    "\n",
    "\n",
    "#k = 10\n",
    "k = 10\n",
    "awaken_set_10 = set([text_1[i:i + k] for i in range(len(text_1) - k + 1)])\n",
    "bb_set_10 = set([text_2[i:i + k] for i in range(len(text_2) - k + 1)])\n",
    "dq_set_10 = set([text_3[i:i + k] for i in range(len(text_3) - k + 1)])\n",
    "niebla_set_10 = set([text_4[i:i + k] for i in range(len(text_4) - k + 1)])\n",
    "odyssey_set_10 = set([text_5[i:i + k] for i in range(len(text_5) - k + 1)])\n",
    "\n",
    "text_sets_10 = [awaken_set_10, bb_set_10, dq_set_10, niebla_set_10, odyssey_set_10]\n",
    "\n",
    "simularities_10 = np.zeros((5,5))\n",
    "\n",
    "for i in range(len(text_sets_10)):\n",
    "    for j in range(len(text_sets_10)):\n",
    "        numer = len(text_sets_10[i].intersection(text_sets_10[j]))\n",
    "        denom = len(text_sets_10[i].union(text_sets_10[j]))\n",
    "        sim = numer/denom\n",
    "        simularities_10[i][j] = sim\n",
    "\n",
    "print(\"k = 10\")\n",
    "print(simularities_10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Implement minhashing with 1000 hash functions on the 4 documents, checking your results against those in part b).\n",
    "\n",
    "- You may choose your own value of $p$ as the modulus of the hash functions.  You are encouraged to use the example code from the minhashing in class notebook to start you out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31131\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awaken</th>\n",
       "      <th>bb</th>\n",
       "      <th>dq</th>\n",
       "      <th>niebla</th>\n",
       "      <th>odyssey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       awaken  bb  dq  niebla  odyssey\n",
       "0           1   0   0       0        0\n",
       "1           1   1   0       0        0\n",
       "2           1   1   0       0        0\n",
       "3           1   1   0       0        0\n",
       "4           1   0   0       0        0\n",
       "...       ...  ..  ..     ...      ...\n",
       "31126       0   0   0       0        1\n",
       "31127       0   0   0       0        1\n",
       "31128       0   0   0       0        1\n",
       "31129       0   0   0       0        1\n",
       "31130       0   0   0       0        1\n",
       "\n",
       "[31131 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working from k = 5\n",
    "k = 6\n",
    "awaken_list_5 = [text_1[i:i + k] for i in range(len(text_1) - k + 1)]\n",
    "bb_list_5 = [text_2[i:i + k] for i in range(len(text_2) - k + 1)]\n",
    "dq_list_5 = [text_3[i:i + k] for i in range(len(text_3) - k + 1)]\n",
    "niebla_list_5 = [text_4[i:i + k] for i in range(len(text_4) - k + 1)]\n",
    "odyssey_list_5 = [text_5[i:i + k] for i in range(len(text_5) - k + 1)]\n",
    "\n",
    "#print(awaken_list_4)\n",
    "# print(len(awaken_list_4))\n",
    "# print(len(bb_list_4))\n",
    "# print(len(dq_list_4))\n",
    "# print(len(niebla_list_4))\n",
    "# print(len(odyssey_list_4))\n",
    "      \n",
    "dup_shingles = [*awaken_list_5, *bb_list_5, *dq_list_5, *niebla_list_5, *odyssey_list_5]\n",
    "\n",
    "# print(len(dup_shingles))\n",
    "\n",
    "shingles = []\n",
    "\n",
    "[shingles.append(s) for s in dup_shingles if s not in shingles]\n",
    "\n",
    "#print(shingles)\n",
    "print(len(shingles))\n",
    "\n",
    "\n",
    "awaken_sig_matrix = []\n",
    "bb_sig_matrix = []\n",
    "dq_sig_matrix = []\n",
    "niebla_sig_matrix = []\n",
    "odyssey_sig_matrix = []\n",
    "\n",
    "for s in shingles:\n",
    "    if s in text_1:\n",
    "        awaken_sig_matrix.append(1)\n",
    "    else:\n",
    "        awaken_sig_matrix.append(0)\n",
    "    \n",
    "    if s in text_2:\n",
    "        bb_sig_matrix.append(1)\n",
    "    else:\n",
    "        bb_sig_matrix.append(0)\n",
    "        \n",
    "    if s in text_3:\n",
    "        dq_sig_matrix.append(1)\n",
    "    else:\n",
    "        dq_sig_matrix.append(0)\n",
    "        \n",
    "    if s in text_4:\n",
    "        niebla_sig_matrix.append(1)\n",
    "    else:\n",
    "        niebla_sig_matrix.append(0)\n",
    "        \n",
    "    if s in text_5:\n",
    "        odyssey_sig_matrix.append(1)\n",
    "    else:\n",
    "        odyssey_sig_matrix.append(0)\n",
    "        \n",
    "d = {'awaken': awaken_sig_matrix, 'bb': bb_sig_matrix, 'dq': dq_sig_matrix, 'niebla': niebla_sig_matrix, 'odyssey': odyssey_sig_matrix}\n",
    "df = pd.DataFrame(data = d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from class nb03\n",
    "def minhash(nhash, dfC):\n",
    "    '''\n",
    "    Takes a number of hash functions to use (nhash) and characteristic matrix (dfC)\n",
    "    '''\n",
    "    # use the \"universal hash\":  (a*x+b) mod p, where a, b are random ints and p > N (= 10 here) is prime\n",
    "    np.random.seed(4022)\n",
    "    Ahash = np.random.choice(range(0,10000), size=nhash)\n",
    "    Bhash = np.random.choice(range(0,10000), size=nhash)\n",
    "    Phash = 61051 #42042\n",
    "\n",
    "    # STEP 2:  initialize signature matrix to all infinities\n",
    "\n",
    "    # initialize the signature matrix\n",
    "    Msig = np.full([nhash, len(dfC.columns)], fill_value=np.inf)\n",
    "\n",
    "    # fill in the signature matrix:\n",
    "\n",
    "    # For each row of the characteristic matrix... \n",
    "    hash_vals = [0]*nhash # initialize\n",
    "    for r in range(len(dfC)):\n",
    "        # STEP 3:  Compute hash values (~permuted row numbers) for that row under each hash function\n",
    "        for h in range(nhash):\n",
    "            hash_vals[h] = (Ahash[h]*r + Bhash[h])%Phash\n",
    "        # STEP 4:  For each column, if there is a 0, do nothing...\n",
    "        for c in range(len(dfC.columns)):\n",
    "            # ... but if there is a 1, replace signature matrix element in that column for each hash fcn \n",
    "            # with the minimum of the hash value in this row, and the current signature matrix element\n",
    "            if dfC.iloc[r,c]==1:\n",
    "                for h in range(nhash):\n",
    "                    if hash_vals[h] < Msig[h,c]:\n",
    "                        Msig[h,c] = hash_vals[h]\n",
    "    return Msig\n",
    "\n",
    "\n",
    "Msig_tmp = minhash(1000, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.104 0.003 0.002 0.074]\n",
      " [0.104 1.    0.002 0.005 0.086]\n",
      " [0.003 0.002 1.    0.093 0.   ]\n",
      " [0.002 0.005 0.093 1.    0.003]\n",
      " [0.074 0.086 0.    0.003 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "hash_simularities = np.zeros((5,5))\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        hash_simularities[i][j] = sum(Msig_tmp[:,i]==Msig_tmp[:,j])/1000\n",
    "        \n",
    "print(hash_simularities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### d) Discussion:\n",
    "\n",
    "Can we detect expected differences here?  Are the two Spanish docuemnts most similar to each other?  Are the two documents by the same author, with the same theme, the most similar?  What kind of alternatives might have captured the structures between these texts?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "We can detect that the English documents have relatively low simularites to the Spanish documents which was expected due to language differences. The two Spanish documents, DQ and Niebla, are more simular to eachother as opposed to the other documents and have the second highest simulurity (.093) overall after Awaken and BB being the most similar (.104) and BB and Odyssey (.086) being the third most. The two documents written by the same author, Awaken and BB, have the highest simularity between two documents at .104. A noticeable trend is that both Awaken and BB are relatively simular to Odyssey with .074 and .086 respective simularites despite being from different others which may suggest similar themes or topics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
